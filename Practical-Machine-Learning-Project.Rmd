## Practical-Machine-Learning-Project

**Author: Nea, Date: 21. Dezember 2014**


###Synopsis
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity. In most kind of using this devices they are interested of the quantified self movement. The goal of this course project is to predict the quality of executing an activity.

In this project we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. "Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curls in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the speciÔ¨Åed execution of the exercise, while the other 4 classes correspond to common mistakes."

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har
(http://groupware.les.inf.puc-rio.br/har) (see the section 5: Detection of mistakes). 

###Preliminary and Libraries
These libraries will be needed for further use.
```{r}
# rm(list=ls()) # clear environment, delete all
setwd("C:/Users/Alfred/Documents/GitHub/PracMachLearning-Project")
suppressMessages(library(dplyr))
suppressMessages(library(caret))
```

### Data Processing
### 1. Loading and preprocessing the data
The input data contains two csv-files, downloaded from the Coursera website. The data are already divided in the training and the testing file. 
```{r}
# load the data
data.train <- read.csv("pml-training.csv", stringsAsFactors=FALSE)
data.test <- read.csv("pml-testing.csv", stringsAsFactors=FALSE)

# convert to a local data frame for easier use
d.train <- tbl_df(data.train)
d.test <- tbl_df(data.test)

```



### 2. Cleaning the data
The first look shows, that there are many data records, which are not complete. There are a lot of NA-Values. 


```{r}
## Preprocessing the data - original
dim(d.train)
head(d.train)
```

The first eight colums are used as identifier for the implementation and realization of the study. They would be removed. 
The filtering process must be done for both - the training and the test set. This will be done in a function.

```{r}
#--------------------------------------------
# function for cleaning of train and test data in identical manner
clearData <- function(df){
      # delete unnecessary columns
      df <- select(df, -(X : num_window)) 
      # delete colums with NA      
      no.na <- !sapply(df, function(x) any(is.na(x)))
      df <- df[, no.na]
      # delete columns with ""-value
      no.zero <- !sapply(df, function(x) any(x==""))
      df <- df[, no.zero]
}
#-------------------------------------------

# tidy up data - containing NA, 0 and Whitespace
d.train <- clearData(d.train)
head(d.train)
dim(d.train)
# the same for the test data
d.test <- clearData(d.test)
head(d.test)
dim(d.test)
```

All of the data records have complete data and are prepared for use.

### 3. Modelling

The next step is to consider a predicting model. 

```{r}
# looking for zero covariates, only numeric variables can be evaluated in this way.
nsv <- nearZeroVar(d.train, saveMetrics=TRUE)
head(nsv)
table(d.train$classe)
```

All variables would be used for modeling. Two models will be sampled -  KNN and Random Forest. The calculation of RF takes a lot time. Maybe the use of the doMC-library for multiple cores will speed-up the calculations. The models are fitted with the training set.

```{r}
# making outcome as factor
d.train$classe <- factor(d.train$classe)
# from: user_caret_2up.pdf, page 34
cvCtrl <- trainControl(method = "cv", repeats = 3)
##--------KNN-----------
mod.KNN  <- train(classe ~ ., data = d.train, method="knn", trControl = cvCtrl)
#------Random Forests ------------------
mod.RF  <- train(classe ~ ., data = d.train, method="rf", trControl = cvCtrl)
# how are the results
mod.KNN
mod.RF
# how accuracy is the model
max(head(mod.KNN$results)$Accuracy)
max(head(mod.RF$results)$Accuracy)
```

The result of the RF-model appears to have the highest value of accuracy. This model will be used for prediction.

### 4. Predictions

The cleaned test dataset in the separated loaded file will be used for prediction. 
```{r}
## prediction -------------------------
pred.RF  <- predict(mod.RF, newdata = d.test)
pred.RF
```

The classifications of the prediction for the 20 test cases should now be evaluated on thew Coursera website. For each test case we submit a text file with a single capital letter corresponding to the prediction for the corresponding problem in the test data set. 

```{r}
##------------------------------
answers <- pred.RF
# from submission website
pml_write_files = function(x){
      n = length(x)
      for(i in 1:n){
            filename = paste0("problem_id_",i,".txt")
            write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
      }
}

pml_write_files(answers)
##-----------------------------
```

All of the submitted predictions were found to be correct.

### 5. Summary

Random forest method is more accurate in the choosen methods. Therefore it is used for predictions on the test records. With the great amount of various measuring it is possible, to predict the accuracy level of performing the exercise with a simple model.

